# Planning document
## Info
[Project requirements and info](https://github.com/fullstack-hy2020/misc/blob/master/project.md)
[General project and course info](https://fullstackopen.com/en/part0/general_info#the-course-exam)

## Scraper Features
1. Support for different types of selectors, such as CSS selectors, XPath expressions, and regular expressions, to extract data from various sources.
2. Ability to paginate through multiple pages of search results or list views, and apply filters or sorting options.
3. Support for user-defined input parameters, such as the search query term, the output file format, and the maximum number of results to fetch.
4. Error handling and retry mechanisms for cases such as network errors, timeouts, or blocked requests.
5. User interface (UI) for configuring and running the scraper, and visualizing the results in real time.
6. Integration with external APIs or databases, allowing users to fetch or store data in other systems.
7. Scheduled or automated scraping, with options such as frequency, time of day, and notifications.
8. Security and privacy features, such as authorization, authentication, and data encryption.
9. Detailed logging and diagnostics, with information about the scraping process, the data sources, and the health of the system.
10. Support for customization and extension, such as plugins, themes, or templates, to allow developers or users to extend the functionality and appearance of the scraper.

## Web frontend
1. Input interface: A web frontend could provide a user interface for the user to provide input parameters, such as the website URL, the type of data to scrape, the selectors to use, etc. This would make the scraper more user-friendly and accessible even to non-technical users.
2. Data visualization: A web frontend could render the scraped data in a more user-friendly way, such as a table, graph or chart, heatmap, or map. This would make it easier for users to analyze and make sense of the data, and might even reveal insights that would not have been apparent with raw data.
3. Filtering and sorting: A web frontend could allow users to filter and sort the scraped data based on various criteria, such as date, location, content type, sentiment, etc. This would help users to narrow down their search and find what they are looking for more quickly.
4. Download and export: A web frontend could allow users to download or export the scraped data in various formats, such as CSV, XLSX, JSON, or PDF. This would enable users to use the data in other applications or share it with others.
5. Schedule and automate: A web frontend could allow users to schedule or automate the scraping process, such as by specifying a frequency, a start/end date, or a set of triggers. This would save users time and effort in executing the scraper manually and maintaining it over time.
6. Notifications and alerts: A web frontend could provide notifications and alerts to users when certain events or thresholds are reached, such as a new article being published, an error occurring, or a quota being exceeded. This would help users stay informed and in control of the scraping process.
